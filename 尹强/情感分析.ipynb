{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 首先加载必用的库\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import jieba # 结巴分词\n",
    "# gensim用来加载预训练word vector\n",
    "from gensim.models import KeyedVectors\n",
    "# 我们使用tensorflow的keras接口来建模\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, GRU, Embedding, LSTM, Bidirectional\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_all = pd.read_csv('C:/Users/86180/Desktop/ChnSentiCorp_htl_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7765, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_all.shape\n",
    "#前5322条为正向，后为负向"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>距离川沙公路较近,但是公交指示不对,如果是\"蔡陆线\"的话,会非常麻烦.建议用别的路线.房间较...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>商务大床房，房间很大，床有2M宽，整体感觉经济实惠不错!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>早餐太差，无论去多少人，那边也不加食品的。酒店应该重视一下这个问题了。房间本身很好。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>宾馆在小街道上，不大好找，但还好北京热心同胞很多~宾馆设施跟介绍的差不多，房间很小，确实挺小...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>CBD中心,周围没什么店铺,说5星有点勉强.不知道为什么卫生间没有电吹风</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                             review\n",
       "0      1  距离川沙公路较近,但是公交指示不对,如果是\"蔡陆线\"的话,会非常麻烦.建议用别的路线.房间较...\n",
       "1      1                       商务大床房，房间很大，床有2M宽，整体感觉经济实惠不错!\n",
       "2      1         早餐太差，无论去多少人，那边也不加食品的。酒店应该重视一下这个问题了。房间本身很好。\n",
       "3      1  宾馆在小街道上，不大好找，但还好北京热心同胞很多~宾馆设施跟介绍的差不多，房间很小，确实挺小...\n",
       "4      1               CBD中心,周围没什么店铺,说5星有点勉强.不知道为什么卫生间没有电吹风"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用gensim加载预训练中文分词embedding\n",
    "cn_model = KeyedVectors.load_word2vec_format('E:/BaiduNetdiskDownload/sgns.zhihu.bigram', binary=False)\n",
    "# 每一个词都对应一个长度为300的向量\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "259883"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cn_model.index2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Users\\86180\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.719 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "平均tokens的长度67.75582743077914\n",
      "最长的评价的tokens的长度1540\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAe1UlEQVR4nO3deZhdVZnv8e9PZhJGEzAjBRjQgBIuZQDRlkGZNXgVCQ108KIRZRLxNgki4hDFFnFE6QhIVKaIIAGEJkyNNmPCHJA2l4RQJCZhCgG5kIS3/9irqJ3iVO1dSZ3ap6p+n+epp85Ze6+133MqOe9Ze+29liICMzOzzryj6gDMzKzxOVmYmVkhJwszMyvkZGFmZoWcLMzMrJCThZmZFXKysNIkXSDp693U1khJr0haJz2/Q9LnuqPt1N6NkiZ0V3tdOO53JD0n6e/d0Nbeklq6I641PP6xkv5S0bEvkfSdKo5ttTlZGACS5kt6TdJySS9JukvS8ZLe+jcSEcdHxLdLtvXRzvaJiAURMTAiVnVD7GdL+l279g+KiGlr23YX4xgBnAaMjoh31dhe6Yd/o6oyKVl5ThaW9/GI2ATYBjgHOB24qLsPImnd7m6zQWwDPB8RS6oOxKy7OVnY20TEsoiYARwBTJC0M6x+akDSIEnXp17IC5L+LOkdkn4LjASuS6eZ/lVSk6SQdJykBcBtubJ84the0n2Slkm6VtKW6Vhv+0be2nuRdCBwBnBEOt7Daftbp7VSXGdKelrSEkm/kbRZ2tYaxwRJC9IppK919N5I2izVX5raOzO1/1FgJjA0xXFJu3oDgBtz21+RNFTSBpJ+LGlh+vmxpA06OPbJkh6XNDzVOzfFvDidItwo/35JOi293kWSPptr5+DUznJJz0r6aqf/INrqvUfSzPT3flLSZ3LbLpF0vqQbUrv3Sto+t33/VGeZpF9I+k9Jn5P0XuACYM/0nryUO+QWHbVnPc/JwjoUEfcBLcCHa2w+LW0bDGxN9oEdEXEMsICslzIwIv4tV+cjwHuBAzo45L8A/wcYCqwEfloixpuA7wJXpuPtUmO3Y9PPPsB2wEDg5+32+RCwI7AfcFb6EKvlZ8BmqZ2PpJg/GxG3AAcBC1Mcx7aL89V22wdGxELga8AewBhgF2AscGb7gyobKzoW+EhEtADfB3ZI9d4NDAPOylV5V4pzGHAccL6kLdK2i4AvpF7kzsBtHbzW/PEHkCXDy4CtgCOBX0jaKbfbkcA3gS2AucCUVHcQcBUwGXgn8CTwwfS+PAEcD9yd3pPNi9qzajhZWJGFwJY1ylcAQ4BtImJFRPw5iicaOzsiXo2I1zrY/tuIeCx9sH4d+IzSAPhaOgo4LyKeiohXyD60xrfr1XwzIl6LiIeBh8k+uFeTYjkCmBwRyyNiPvBD4Ji1jO1bEbEkIpaSfTjm25Ok88gS7D4RsVSSgM8Dp0bECxGxnCxhjs/VW5HaXRERfwJeIUuGrdtGS9o0Il6MiAdKxHkoMD8ifh0RK1OdPwCfzu1zdUTcFxErgUvJEhnAwcCciLg6bfspUOYCgI7aswo4WViRYcALNcp/QPZt72ZJT0maVKKtZ7qw/WlgPWBQqSg7NzS1l297XbIeUav8h9c/yHof7Q0C1q/R1rBujm1o7vnmwETgexGxLJUNBjYGZqfTgC8BN6XyVs+nD9lW+df0KbIP8KfT6aA9S8S5DbB76/HSMY8i68G06ug9HErub5u+VJQZ6C/zN7Ee4mRhHZL0AbIPwrddqZK+WZ8WEdsBHwe+Imm/1s0dNFnU8xiRezyS7Bvwc8CrZB+OrXGtw+ofjEXtLiT7sMu3vRJYXFCvvedSTO3berZk/Vpx1optYe75i2Tf6n8taa9cHK8BO0XE5ulns4go9WEaEfdHxDiy00l/BKaXqPYM8J+5422eTht9sUTdRcDw1iepZzQ8t91TX/cCThb2NpI2lXQocAXwu4h4tMY+h0p6d/qP/zKwKv1A9iG83Roc+mhJoyVtDHwLuCpdWvvfwIaSDpG0Htk5/fwg8GKgSbnLfNu5HDhV0raSBtI2xrGyg/1rSrFMB6ZI2kTSNsBXgN91XnO1ON/ZOriei+1MSYPTuf2z2rcXEXeQfYu/RtLuEfEm8CvgR5K2ApA0TFJHY0FvkbS+pKMkbRYRK2j72xW5HthB0jGS1ks/H+hkbCfvBuB9kg5Lp/5OYPUeyWJguKT1S7RlFXGysLzrJC0n+xb5NeA84LMd7DsKuIXsXPjdwC/ShxrA98g+AF8qe6VN8lvgErLTDxsCJ0N2dRbwJeBCsm/xr7L6aYzfp9/PS6p1/v3i1PadwDzg/wMndSGuvJPS8Z8i63FdltovFBF/JUsOT6X3ZijwHWAW8AjwKPBAKmtfdybZ32KGpN3ILmueC9wj6WWyv8WO7et14Bhgfqp3PHB0idiXA/uTjYssJPsbfZ/Vk3ZHdZ8DDgf+DXgeGE32ml9Pu9wGzAH+Lum5kq/Bepi8+JGZ9aTUA2wBjoqI26uOx8pxz8LM6k7SAZI2T/eQnAEIuKfisKwLnCzMrCfsCfw/ssH5jwOHdXIJtTUgn4YyM7NC7lmYmVmhXj2h26BBg6KpqanqMMzMepXZs2c/FxGDi/ds06uTRVNTE7Nmzao6DDOzXkXS08V7rc6noczMrJCThZmZFXKyMDOzQk4WZmZWyMnCzMwKOVmYmVmhuiULSRsqW0/5YUlzJH0zlW+Z1vH9W/q9Ra7OZElz01q9hdMtm5lZz6hnz+J1YN+0JvIY4EBJewCTgFsjYhRwa3qOpNFk0x/vBBxItr5vdyypaWZma6luySIyr6Sn66WfAMYB01L5NOCw9HgccEVEvB4R88jm6h9br/jMzKy8ut7BnXoGs4F3A+dHxL2Sto6IRQARsah1pS+y5TvzUxa3UGNtY0kTydYkZuTIkfUM3yrSNOmGtx7PP+eQCiMxs1Z1HeCOiFURMYZsvd2xknbuZHfVaqJGm1MjojkimgcP7tLUJmZmtoZ65GqoiHgJuINsLGKxpCEA6feStFsLMCJXbTirL1xvZmYVqefVUIMlbZ4ebwR8FPgrMAOYkHabAFybHs8AxkvaQNK2ZGs831ev+MzMrLx6jlkMAaalcYt3ANMj4npJdwPTJR0HLCBbyJ2ImCNpOvA4sBI4ISJW1TE+MzMrqW7JIiIeAXatUf48sF8HdaYAU+oVk5mZrRnfwW1mZoWcLMzMrJCThZmZFXKysIbWNOmG1W7SM7NqOFmYmVkhJwszMyvkZGFmZoWcLMzMrJCThZmZFXKyMDOzQk4WZmZWyMnCzMwKOVmYmVkhJwszMyvkZGFmZoWcLMzMrJCThZmZFXKyMDOzQk4WZmZWyMnCzMwKOVmYmVkhJwvrFlWvaFf18c36OicLMzMr5GRhZmaFnCzMzKxQ3ZKFpBGSbpf0hKQ5kk5J5WdLelbSQ+nn4FydyZLmSnpS0gH1is3MzLpm3Tq2vRI4LSIekLQJMFvSzLTtRxFxbn5nSaOB8cBOwFDgFkk7RMSqOsZoZmYl1K1nERGLIuKB9Hg58AQwrJMq44ArIuL1iJgHzAXG1is+MzMrr0fGLCQ1AbsC96aiEyU9IuliSVuksmHAM7lqLdRILpImSpoladbSpUvrGbaZmSV1TxaSBgJ/AL4cES8DvwS2B8YAi4Aftu5ao3q8rSBiakQ0R0Tz4MGD6xO0mZmtpq7JQtJ6ZIni0oi4GiAiFkfEqoh4E/gVbaeaWoARuerDgYX1jM/MzMqp59VQAi4CnoiI83LlQ3K7fRJ4LD2eAYyXtIGkbYFRwH31is/MzMqr59VQewHHAI9KeiiVnQEcKWkM2Smm+cAXACJijqTpwONkV1Kd4CuhrKtap/yYf84hFUdi1rfULVlExF+oPQ7xp07qTAGm1CsmMzNbM76D28zMCjlZmJlZIScLMzMrVM8BbrOGkl/vwgPgZl3jnoWZmRVysrBex6vimfU8n4ayXsHJwaxa7lmYmVkhJwszMyvkZGFmZoWcLMzMrJAHuK3P8+C42dpzz8LMzAo5WZiZWSEnCzMzK+RkYWZmhZwszMyskJOFmZkVcrKwuvGEf2Z9R+F9FpIGAK9FxJuSdgDeA9wYESvqHp1ZnXmNC7NyytyUdyfwYUlbALcCs4AjgKPqGZj1bbV6HP6wNmtcZU5DKSL+Afxv4GcR8UlgdH3DMjOzRlKmZyFJe5L1JI7rQj2zynisxKx7lelZnAJMBq6JiDmStgNur29YZmbWSAp7CBFxJ9m4Revzp4CT6xmU9U/uDZg1rjJXQ+0AfBVoyu8fEfvWLyzrS1qTgAewzXqvMmMPvwcuAC4EVpVtWNII4DfAu4A3gakR8RNJWwJXkiWf+cBnIuLFVGcy2bjIKuDkiPiP0q/EzMzqpkyyWBkRv1yDtlcCp0XEA5I2AWZLmgkcC9waEedImgRMAk6XNBoYD+wEDAVukbRDRJROUGZrwz0gs46VSRbXSfoScA3wemthRLzQWaWIWAQsSo+XS3oCGAaMA/ZOu00D7gBOT+VXRMTrwDxJc4GxwN1deD3WwDwmYdZ7lUkWE9Lv/5srC2C7sgeR1ATsCtwLbJ0SCRGxSNJWabdhwD25ai2prH1bE4GJACNHjiwbgpmZrYUyV0NtuzYHkDQQ+APw5Yh4WVKHu9Y6fI14pgJTAZqbm9+23czMul/hfRaSNpZ0pqSp6fkoSYeWaVzSemSJ4tKIuDoVL5Y0JG0fAixJ5S3AiFz14cDCci/DzMzqqcxNeb8G3gA+mJ63AN8pqqSsC3ER8EREnJfbNIO2U1sTgGtz5eMlbSBpW2AUcF+J+MzMrM7KjFlsHxFHSDoSICJeUyfnknL2Ao4BHpX0UCo7AzgHmC7pOGABcHhqd46k6cDjZFdSneAroczMGkOZZPGGpI1I4weStid3VVRHIuIv1B6HANivgzpTgCklYjIzsx5UJll8A7gJGCHpUrIew7H1DMrMzBpLmWQxm2x68j3IegqnAJvUMygzM2ssZQa4rwNWRMQNEXE9MDiVmZlZP1EmWXyX7C7uAZJ2A64Cjq5vWGZm1kjK3JR3Q7pfYibZ6afDIuJvdY/MzMwaRofJQtLPWP0O6k2Bp4CTJBERXtPCKrU2c015niqzrumsZzGr3fPZ9QzEzMwaV4fJIiKmtT6WtD6wQ3r6ZESsqHdgZmbWOMqslLc32VTi88kunR0haUJabtXMzPqBMvdZ/BDYPyKehLeWWb0c2K2egZmZWeMokyzWa00UABHx3+nqKOsH+uPqcfnB7/70us06UyZZzJJ0EfDb9PwoPNhtZtavlEkWXwROAE4mG7O4Ezi/nkGZmVljKZMsjk/rUby1JoWkU4Cf1C0qMzNrKGXX4G6fGI6tUWbWZ3kcw/q7zu7gPhL4Z2BbSTNymzYBnq93YGZm1jg661ncBSwCBpFdPttqOfBIPYMyM7PG0tkd3E8DTwN79lw4ZmbWiMpMUW5mZv1cmQFu62c8I6uZtddhz0LSren393suHDMza0Sd9SyGSPoI8AlJV5DdkPeWiHigrpFZj3OP4u38nphlOksWZwGTgOHkbshLAti3XkGZmVlj6exqqKuAqyR9PSK+3YMxmTW0/ji5olmZNbi/LekTwD+lojsi4vr6hmVmZo2k8NJZSd8DTgEeTz+npLKiehdLWiLpsVzZ2ZKelfRQ+jk4t22ypLmSnpR0wJq9HDMzq4cyl84eAoyJiDcBJE0DHgQmF9S7BPg58Jt25T+KiHPzBZJGA+OBnYChwC2SdoiIVSXiMzOzOit7U97muceblamQll19oWT744ArIuL1iJgHzAXGlqxrZmZ1ViZZfA94UNIlqVcxG/juWhzzREmPpNNUW6SyYcAzuX1aUtnbSJooaZakWUuXLl2LMMzMrKzCZBERlwN7AFennz0j4oo1PN4vge2BMWSTFLZOUKga+0YH8UyNiOaIaB48ePAahmFmZl1RarqPiFgEzCjcsbidxa2PJf0KaL2qqgUYkdt1OLBwbY9nZmbdo0cnEpQ0JPf0k0DrlVIzgPGSNpC0LTAKuK8nYzMzs47VbSJBSZcDewODJLUA3wD2ljSG7BTTfOALABExR9J0sktzVwIn+EooM7PGUSpZSFoH2Dq/f0Qs6KxORBxZo/iiTvafAkwpE4+ZmfWswmQh6SSyXsFi4M1UHMD76xiXWa/iKUCsryvTszgF2DEivO62mVk/VWaA+xlgWb0Dsd6jadINnrrbrJ/psGch6Svp4VPAHZJuAF5v3R4R7actNzOzPqqz01CbpN8L0s/66Qc6uGHOeh/3EMysjM7Ws/gmgKTDI+L3+W2SDq93YNZYnFTM+rcyYxa1ZpctmnHWzMz6kM7GLA4CDgaGSfppbtOmZDfOmZlZP9HZmMVCYBbwCbKZZlstB06tZ1BmZtZYOhuzeBh4WNJlEbGiB2OyHuAxCDPrijI35T0gqf3VT8vIeh3f8c16ZmZ9X5lkcSOwCrgsPR9Ptv7EMrKlUz9el8jMzKxhlEkWe0XEXrnnj0r6r4jYS9LR9QrMzMwaR5lLZwdK2r31iaSxwMD01FdFmZn1A2V6Fp8DLpY0kOz008vA5yQNIFuf28zM+rjCZBER9wPvk7QZoIh4Kbd5er0CMzOzxlFmPYsNgE8BTcC6kgCIiG/VNTJreL781qz/KHMa6lqyK59mk5t11qwWJxCzvqlMshgeEQfWPRIzM2tYZa6GukvS++oeiZmZNawyPYsPAcdKmkd2GkpARITX4DYz6yfKJIuD6h6FmZk1tMLTUBHxNDAC2Dc9/keZemZm1ncUfuhL+gZwOm0LHq0H/K6eQZmZWWMp00P4JNmaFq8CRMRC2tbnNjOzfqBMsngjIgIIgDTNh5mZ9SNlksV0Sf8ObC7p88AtwK+KKkm6WNISSY/lyraUNFPS39LvLXLbJkuaK+lJSQesyYsxM7P6KDM31LmSPkY2geCOwFkRMbNE25cAPwd+kyubBNwaEedImpSeny5pNNk6GTsBQ4FbJO0QEau69GqskO+wNrM1UebSWVJyKJMg8nXulNTUrngcsHd6PA24g2zwfBxwRUS8DsyTNBcYC9zdlWOamVl9dJgsJC0njVO030R2U96ma3C8rSNiEVkDiyRtlcqHAffk9mtJZbXimghMBBg5cuQahGBWf/ke3PxzDqkwErPu0WGyiIievOJJtUKotWNETAWmAjQ3N9fcx8zMuldP31y3WNIQgPR7SSpvIbvxr9VwYGEPx2ZmZh0oNWbRjWYAE4Bz0u9rc+WXSTqPbIB7FHBfD8dmttZ8AYH1VXVLFpIuJxvMHiSpBfgGWZKYLuk4YAFwOEBEzJE0HXicbF3vE3wllJlZ41B2v13v1NzcHLNmzao6jF7F33yr44FuaxSSZkdEc1fqeEJAMzMr5GRhZmaFnCzMzKyQk4WZmRVysjAzs0JOFmZmVsjJwszMCjlZmJlZoZ6e7sOs3/JMtNabuWdhZmaFnCzMzKyQk4WZmRVysjAzs0JOFmZmVsjJwszMCjlZmJlZIScLMzMr5GRhZmaFnCzMzKyQk4WZmRVysjAzs0JOFmYVaJp0w2oTC5o1Os86a1Yhz0RrvYV7FmZmVsjJwszMClVyGkrSfGA5sApYGRHNkrYErgSagPnAZyLixSriMzOz1VXZs9gnIsZERHN6Pgm4NSJGAbem52Zm1gAa6TTUOGBaejwNOKy6UMzMLK+qZBHAzZJmS5qYyraOiEUA6fdWtSpKmihplqRZS5cu7aFwzcz6t6ound0rIhZK2gqYKemvZStGxFRgKkBzc3PUK0AzM2tTSbKIiIXp9xJJ1wBjgcWShkTEIklDgCVVxGZWtVo36/keDKtaj5+GkjRA0iatj4H9gceAGcCEtNsE4Nqejq2v8V3CZtZdquhZbA1cI6n1+JdFxE2S7gemSzoOWAAcXkFsZpVxYrdG1uPJIiKeAnapUf48sF9Px2NmZsUa6dJZM+sCn2a0nuRkYWZmhZwszMyskJOFmZkVcrIwM7NCXvzIrBfwIklWNfcszMyskJOFmZkVcrIwM7NCHrPoY3yTVt/nv7FVwT0LMzMr5GRhZmaFfBrKrJfzZbXWE9yzMDOzQk4WZn2IZ6K1enGyMDOzQk4WZmZWyMnCzMwKOVmYmVkhJwszMyvkZGFmZoV8U55ZH1Tr8tlaN+yV3c/MyaKX8F26ZlYlJ4tezAnEusL/XmxteMzCzMwKuWdh1g+VnRKkdT/3RKzhkoWkA4GfAOsAF0bEOT0dw5p01/2fysz6soZKFpLWAc4HPga0APdLmhERj9fa/9Fnl/W6D+nO4u1tr8X6plq9Dv/btEYbsxgLzI2IpyLiDeAKYFzFMZlZDWVnuO3u/awaioiqY3iLpE8DB0bE59LzY4DdI+LE3D4TgYnp6c7AYz0eaGMaBDxXdRANwu9FG78XbfxetNkxIjbpSoWGOg0FqEbZatksIqYCUwEkzYqI5p4IrNH5vWjj96KN34s2fi/aSJrV1TqNdhqqBRiRez4cWFhRLGZmljRasrgfGCVpW0nrA+OBGRXHZGbW7zXUaaiIWCnpROA/yC6dvTgi5nRSZWrPRNYr+L1o4/eijd+LNn4v2nT5vWioAW4zM2tMjXYayszMGpCThZmZFeq1yULSgZKelDRX0qSq46mKpBGSbpf0hKQ5kk6pOqYqSVpH0oOSrq86lqpJ2lzSVZL+mv597Fl1TFWRdGr6//GYpMslbVh1TD1F0sWSlkh6LFe2paSZkv6Wfm9R1E6vTBa5aUEOAkYDR0oaXW1UlVkJnBYR7wX2AE7ox+8FwCnAE1UH0SB+AtwUEe8BdqGfvi+ShgEnA80RsTPZxTPjq42qR10CHNiubBJwa0SMAm5NzzvVK5MFnhbkLRGxKCIeSI+Xk30gDKs2qmpIGg4cAlxYdSxVk7Qp8E/ARQAR8UZEvFRpUNVaF9hI0rrAxvSj+7ci4k7ghXbF44Bp6fE04LCidnprshgGPJN73kI//YDMk9QE7ArcW3EoVfkx8K/AmxXH0Qi2A5YCv06n5S6UNKDqoKoQEc8C5wILgEXAsoi4udqoKrd1RCyC7AsnsFVRhd6aLAqnBelvJA0E/gB8OSJerjqenibpUGBJRMyuOpYGsS7wv4BfRsSuwKuUONXQF6Xz8eOAbYGhwABJR1cbVe/TW5OFpwXJkbQeWaK4NCKurjqeiuwFfELSfLLTkvtK+l21IVWqBWiJiNZe5lVkyaM/+igwLyKWRsQK4GrggxXHVLXFkoYApN9Liir01mThaUESSSI7L/1ERJxXdTxViYjJETE8IprI/j3cFhH99ttjRPwdeEbSjqloP6DmujD9wAJgD0kbp/8v+9FPB/tzZgAT0uMJwLVFFRpquo+y1mBakL5sL+AY4FFJD6WyMyLiT9WFZA3iJODS9IXqKeCzFcdTiYi4V9JVwANkVw8+SD+a+kPS5cDewCBJLcA3gHOA6ZKOI0umhxe24+k+zMysSG89DWVmZj3IycLMzAo5WZiZWSEnCzMzK+RkYWZmhZwszABJd0hq7oHjnJxmgL20XfkYSQeXqH+2pK/WL0Kz2pwszNZSmpyurC8BB0fEUe3KxwCFycKsKk4W1mtIakrfyn+V1ia4WdJGadtbPQNJg9K0H0g6VtIfJV0naZ6kEyV9JU2ud4+kLXOHOFrSXWnNg7Gp/oC0HsD9qc64XLu/l3Qd8LZJ6dIxHks/X05lF5BN8DdD0qm5fdcHvgUcIekhSUek9Qb+KOmRFOf7axzj85JulLSRpKMl3Zfq/3uaxh9Jr0iaIunh1M7WqfzwFNvDku5c27+N9X1OFtbbjALOj4idgJeAT5WoszPwz2RT208B/pEm17sb+JfcfgMi4oNk3/4vTmVfI5s65APAPsAPcrO37glMiIh98weTtBvZ3dK7k60x8nlJu0bE8WRzmO0TET9q3T9Ns38WcGVEjImIK4FvAg9GxPuBM4DftDvGicDHyaaWbgKOAPaKiDHAKqC15zIAuCcidgHuBD6fys8CDkjlnyjxHlo/1yun+7B+bV5EPJQezyb7oCxye1rrY7mkZcB1qfxRIP+N/XLI5v+XtKmkzYH9ySYobB0n2BAYmR7PjIj26wQAfAi4JiJeBZB0NfBhsmkmyvoQKRFGxG2S3ilps7TtGLKJAg+LiBWS9gN2A+7Ppj5iI9omhnsDaF01cDbwsfT4v4BLJE0nm1jPrFNOFtbbvJ57vIrsgxGyOX9ae8rtl8zM13kz9/xNVv8/0H7umyCbDv9TEfFkfoOk3cmm/a6l1hT6XdXZNPyPkY1xDAfmpX2nRcTkGnVWRNucPqtIrzcijk+v4RDgIUljIuL5bojb+iifhrK+Yj7Zt2uAT69hG0cASPoQ2QI5y8gmqzwpzVaKpF1LtHMncFia5XQA8EngzwV1lgObtGvjqHTMvYHncuuUPAh8gWzsYyjZspiflrRV2n9LSdt0djBJ20fEvRFxFvAcq0/5b/Y2ThbWV5wLfFHSXcCgNWzjxVT/AuC4VPZtYD3gEWUL3n+7qJG0zO0lwH1kqxZeGBFFp6BuB0a3DnADZwPNkh4hmyF0Qn7niPgL8FXgBrJTTmcCN6f9ZwJDCo73A0mPptd0J/Bw0euy/s2zzpqZWSH3LMzMrJCThZmZFXKyMDOzQk4WZmZWyMnCzMwKOVmYmVkhJwszMyv0PyB5adQi9mMPAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#存储所有评价，每例评价为一条string\n",
    "train_texts_orig = list(pd_all.review)\n",
    "#前\n",
    "train_tokens = []\n",
    "\n",
    "#分词和tokenize\n",
    "#首先我们去掉每个样本的标点符号，然后用jieba分词，jieba分词返回一个生成器，没法直接进行tokenize，\n",
    "#所以我们将分词结果转换成一个list，并将它索引化，这样每一例评价的文本变成一段索引数字，对应着预训练词向量模型中的词。\n",
    "for text in train_texts_orig:\n",
    "    # 去掉标点\n",
    "    text = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\'%]+|[+——！，。？、~@#￥%……&*（）]+\", \"\", text)\n",
    "    # 结巴分词\n",
    "    cut = jieba.cut(text)\n",
    "    # 结巴分词的输出结果为一个生成器\n",
    "    # 把生成器转换为list\n",
    "    cut_list = [ i for i in cut ]\n",
    "    for i, word in enumerate(cut_list):\n",
    "        try:\n",
    "            # 将词转换为索引index\n",
    "            cut_list[i] = cn_model.vocab[word].index\n",
    "        except KeyError:\n",
    "            # 如果词不在字典中，则输出0\n",
    "            cut_list[i] = 0\n",
    "    train_tokens.append(cut_list)\n",
    "\n",
    "# 获得所有tokens的长度\n",
    "num_tokens = [ len(tokens) for tokens in train_tokens ]\n",
    "num_tokens = np.array(num_tokens)\n",
    "\n",
    "\n",
    "# 最长的评价tokens的长度\n",
    "np.max(num_tokens)\n",
    "print('平均tokens的长度{}'.format(np.mean(num_tokens)))\n",
    "print('最长的评价的tokens的长度{}'.format(np.max(num_tokens)))\n",
    "plt.hist(np.log(num_tokens), bins=100)\n",
    "plt.xlim((0, 10))\n",
    "plt.ylabel('length of tokens')\n",
    "plt.xlabel('number of tokens')\n",
    "plt.title('Distribution of tokens length')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "222"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 取tokens平均值并加上两个tokens的标准差，\n",
    "# 假设tokens长度的分布为正态分布，则max_tokens这个值可以涵盖95%左右的样本\n",
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "max_tokens = int(max_tokens)\n",
    "max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9563425627817128"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum( num_tokens < max_tokens ) / len(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # 初始化embedding_matrix，之后在keras上进行应用\n",
    "# num_words = 259883\n",
    "# embedding_dim = 300\n",
    "# embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "# # embedding_matrix为一个 [num_words，embedding_dim] 的矩阵\n",
    "# # 维度为 259883 * 300\n",
    "# for i in range(num_words):\n",
    " \n",
    "#     embedding_matrix[i, :] = cn_model[cn_model.index2word[i]]\n",
    "#     print(\"progress:{0}%\".format(round((i + 1) * 100 / num_words)), end=\"\\r\")\n",
    "#     embedding_matrix = embedding_matrix.astype('float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_matrix_df = pd.DataFrame(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_matrix_df.to_csv('embedding_matrix_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix_df=pd.read_csv('embedding_matrix_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.801784</td>\n",
       "      <td>-0.165340</td>\n",
       "      <td>0.030508</td>\n",
       "      <td>-0.117380</td>\n",
       "      <td>0.344886</td>\n",
       "      <td>-0.619519</td>\n",
       "      <td>-0.163887</td>\n",
       "      <td>-0.256985</td>\n",
       "      <td>-0.136003</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055162</td>\n",
       "      <td>0.126540</td>\n",
       "      <td>0.464693</td>\n",
       "      <td>0.476686</td>\n",
       "      <td>0.387382</td>\n",
       "      <td>-0.822759</td>\n",
       "      <td>-0.504953</td>\n",
       "      <td>0.106525</td>\n",
       "      <td>0.553436</td>\n",
       "      <td>0.436650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.651747</td>\n",
       "      <td>0.535970</td>\n",
       "      <td>0.340271</td>\n",
       "      <td>0.070466</td>\n",
       "      <td>0.335049</td>\n",
       "      <td>-0.681975</td>\n",
       "      <td>-0.573658</td>\n",
       "      <td>0.318913</td>\n",
       "      <td>-0.042395</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015890</td>\n",
       "      <td>0.604637</td>\n",
       "      <td>-0.400023</td>\n",
       "      <td>0.038662</td>\n",
       "      <td>0.473588</td>\n",
       "      <td>-0.735384</td>\n",
       "      <td>-1.047046</td>\n",
       "      <td>0.805399</td>\n",
       "      <td>0.104593</td>\n",
       "      <td>0.193694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.412321</td>\n",
       "      <td>0.228261</td>\n",
       "      <td>0.207114</td>\n",
       "      <td>-0.245406</td>\n",
       "      <td>-0.292829</td>\n",
       "      <td>-0.082571</td>\n",
       "      <td>-0.882113</td>\n",
       "      <td>-0.141292</td>\n",
       "      <td>0.057696</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026220</td>\n",
       "      <td>0.303933</td>\n",
       "      <td>0.357741</td>\n",
       "      <td>-0.184716</td>\n",
       "      <td>-0.497120</td>\n",
       "      <td>-0.390048</td>\n",
       "      <td>-0.258891</td>\n",
       "      <td>0.808777</td>\n",
       "      <td>0.056751</td>\n",
       "      <td>0.452374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.505105</td>\n",
       "      <td>-0.412067</td>\n",
       "      <td>-0.041826</td>\n",
       "      <td>-0.055158</td>\n",
       "      <td>-0.826903</td>\n",
       "      <td>-0.868761</td>\n",
       "      <td>-0.689815</td>\n",
       "      <td>-0.074329</td>\n",
       "      <td>-0.741576</td>\n",
       "      <td>...</td>\n",
       "      <td>0.125112</td>\n",
       "      <td>-0.677940</td>\n",
       "      <td>0.548021</td>\n",
       "      <td>-0.542195</td>\n",
       "      <td>-0.270064</td>\n",
       "      <td>0.252053</td>\n",
       "      <td>0.215630</td>\n",
       "      <td>1.569453</td>\n",
       "      <td>-0.358775</td>\n",
       "      <td>-0.300759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.577619</td>\n",
       "      <td>-0.821557</td>\n",
       "      <td>-0.440517</td>\n",
       "      <td>0.049569</td>\n",
       "      <td>0.754438</td>\n",
       "      <td>-0.617761</td>\n",
       "      <td>0.273806</td>\n",
       "      <td>-0.302287</td>\n",
       "      <td>0.510386</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.267514</td>\n",
       "      <td>0.553856</td>\n",
       "      <td>0.764344</td>\n",
       "      <td>0.775949</td>\n",
       "      <td>0.836743</td>\n",
       "      <td>0.247767</td>\n",
       "      <td>-0.702770</td>\n",
       "      <td>-0.229911</td>\n",
       "      <td>-0.003598</td>\n",
       "      <td>0.074756</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 301 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0         0         1         2         3         4         5  \\\n",
       "0           0 -0.801784 -0.165340  0.030508 -0.117380  0.344886 -0.619519   \n",
       "1           1 -0.651747  0.535970  0.340271  0.070466  0.335049 -0.681975   \n",
       "2           2 -0.412321  0.228261  0.207114 -0.245406 -0.292829 -0.082571   \n",
       "3           3 -0.505105 -0.412067 -0.041826 -0.055158 -0.826903 -0.868761   \n",
       "4           4  0.577619 -0.821557 -0.440517  0.049569  0.754438 -0.617761   \n",
       "\n",
       "          6         7         8  ...       290       291       292       293  \\\n",
       "0 -0.163887 -0.256985 -0.136003  ...  0.055162  0.126540  0.464693  0.476686   \n",
       "1 -0.573658  0.318913 -0.042395  ... -0.015890  0.604637 -0.400023  0.038662   \n",
       "2 -0.882113 -0.141292  0.057696  ... -0.026220  0.303933  0.357741 -0.184716   \n",
       "3 -0.689815 -0.074329 -0.741576  ...  0.125112 -0.677940  0.548021 -0.542195   \n",
       "4  0.273806 -0.302287  0.510386  ... -0.267514  0.553856  0.764344  0.775949   \n",
       "\n",
       "        294       295       296       297       298       299  \n",
       "0  0.387382 -0.822759 -0.504953  0.106525  0.553436  0.436650  \n",
       "1  0.473588 -0.735384 -1.047046  0.805399  0.104593  0.193694  \n",
       "2 -0.497120 -0.390048 -0.258891  0.808777  0.056751  0.452374  \n",
       "3 -0.270064  0.252053  0.215630  1.569453 -0.358775 -0.300759  \n",
       "4  0.836743  0.247767 -0.702770 -0.229911 -0.003598  0.074756  \n",
       "\n",
       "[5 rows x 301 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pad = pad_sequences(train_tokens, maxlen=max_tokens, padding='pre', truncating='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "         1117, 153609,   4549,      0,     86,   3816,   8737,     10,\n",
       "           39,     91,      4,      0,    425,     30,    175,   1843,\n",
       "          669,     71,    901,   3216,   1487,  94816])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pad[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target = np.concatenate( (np.ones(5322), np.zeros(7765-5322)) )\n",
    "# 进行训练和测试样本的分割\n",
    " \n",
    "# 90%的样本用来训练，剩余10%用来测试\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_pad, train_target, test_size=0.1, random_state=12)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 普通LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#用LSTM对样本进行分类训练\n",
    "model1=Sequential()\n",
    "model1.add(Embedding(num_words,embedding_dim,\n",
    "                    weights=[embedding_matrix],\n",
    "                   input_length=max_tokens,\n",
    "                   trainable=False))\n",
    "model1.add(LSTM(units=64))\n",
    "model1.add(Dropout(0.5))\n",
    "model1.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "#优化器，损失函数二元交叉熵，评估标准acc\n",
    "model1.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_15 (Embedding)     (None, 222, 300)          77964900  \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "module_wrapper_43 (ModuleWra (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "module_wrapper_44 (ModuleWra (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 78,058,405\n",
      "Trainable params: 93,505\n",
      "Non-trainable params: 77,964,900\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "50/50 [==============================] - 27s 478ms/step - loss: 0.5345 - acc: 0.7284 - val_loss: 0.4325 - val_acc: 0.8112\n",
      "Epoch 2/50\n",
      "50/50 [==============================] - 24s 473ms/step - loss: 0.4377 - acc: 0.7949 - val_loss: 0.4059 - val_acc: 0.8312\n",
      "Epoch 3/50\n",
      "50/50 [==============================] - 24s 475ms/step - loss: 0.3971 - acc: 0.8286 - val_loss: 0.4135 - val_acc: 0.7969\n",
      "Epoch 4/50\n",
      "50/50 [==============================] - 24s 477ms/step - loss: 0.3660 - acc: 0.8431 - val_loss: 0.4617 - val_acc: 0.7825\n",
      "Epoch 5/50\n",
      "50/50 [==============================] - 24s 476ms/step - loss: 0.3422 - acc: 0.8572 - val_loss: 0.4296 - val_acc: 0.8197\n",
      "Epoch 6/50\n",
      "50/50 [==============================] - 24s 477ms/step - loss: 0.3599 - acc: 0.8375 - val_loss: 0.8222 - val_acc: 0.6853\n",
      "Epoch 7/50\n",
      "50/50 [==============================] - 24s 480ms/step - loss: 0.3241 - acc: 0.8650 - val_loss: 0.5935 - val_acc: 0.7167\n",
      "Epoch 8/50\n",
      "50/50 [==============================] - 24s 480ms/step - loss: 0.3091 - acc: 0.8723 - val_loss: 0.3584 - val_acc: 0.8555\n",
      "Epoch 9/50\n",
      "50/50 [==============================] - 24s 480ms/step - loss: 0.2941 - acc: 0.8728 - val_loss: 0.3209 - val_acc: 0.8584\n",
      "Epoch 10/50\n",
      "50/50 [==============================] - 24s 479ms/step - loss: 0.2769 - acc: 0.8839 - val_loss: 0.3759 - val_acc: 0.8498\n",
      "Epoch 11/50\n",
      "50/50 [==============================] - 24s 481ms/step - loss: 0.2618 - acc: 0.8931 - val_loss: 0.3408 - val_acc: 0.8441\n",
      "Epoch 12/50\n",
      "50/50 [==============================] - 25s 492ms/step - loss: 0.2547 - acc: 0.8930 - val_loss: 0.3220 - val_acc: 0.8612\n",
      "Epoch 13/50\n",
      "50/50 [==============================] - 24s 489ms/step - loss: 0.2378 - acc: 0.9052 - val_loss: 0.5349 - val_acc: 0.8355\n",
      "Epoch 14/50\n",
      "50/50 [==============================] - 24s 479ms/step - loss: 0.2267 - acc: 0.9089 - val_loss: 0.3535 - val_acc: 0.8813\n",
      "Epoch 15/50\n",
      "50/50 [==============================] - 24s 481ms/step - loss: 0.2085 - acc: 0.9162 - val_loss: 0.4518 - val_acc: 0.8541\n",
      "Epoch 16/50\n",
      "50/50 [==============================] - 24s 478ms/step - loss: 0.2002 - acc: 0.9208 - val_loss: 0.3370 - val_acc: 0.8684\n",
      "Epoch 17/50\n",
      "50/50 [==============================] - 24s 488ms/step - loss: 0.1990 - acc: 0.9213 - val_loss: 0.3781 - val_acc: 0.8512\n",
      "Epoch 18/50\n",
      "50/50 [==============================] - 24s 479ms/step - loss: 0.1646 - acc: 0.9346 - val_loss: 0.5028 - val_acc: 0.8755\n",
      "Epoch 19/50\n",
      "50/50 [==============================] - 25s 492ms/step - loss: 0.1734 - acc: 0.9326 - val_loss: 0.3842 - val_acc: 0.8741\n",
      "Epoch 20/50\n",
      "50/50 [==============================] - 24s 486ms/step - loss: 0.1419 - acc: 0.9490 - val_loss: 0.3825 - val_acc: 0.8541\n",
      "Epoch 21/50\n",
      "50/50 [==============================] - 24s 478ms/step - loss: 0.1409 - acc: 0.9451 - val_loss: 0.4270 - val_acc: 0.8770\n",
      "Epoch 22/50\n",
      "50/50 [==============================] - 24s 483ms/step - loss: 0.1187 - acc: 0.9556 - val_loss: 0.5543 - val_acc: 0.8469\n",
      "Epoch 23/50\n",
      "50/50 [==============================] - 24s 486ms/step - loss: 0.1149 - acc: 0.9591 - val_loss: 0.4704 - val_acc: 0.8484\n",
      "Epoch 24/50\n",
      "50/50 [==============================] - 24s 482ms/step - loss: 0.1088 - acc: 0.9579 - val_loss: 0.4839 - val_acc: 0.8770\n",
      "Epoch 25/50\n",
      "50/50 [==============================] - 24s 475ms/step - loss: 0.0987 - acc: 0.9649 - val_loss: 0.4618 - val_acc: 0.8655\n",
      "Epoch 26/50\n",
      "50/50 [==============================] - 24s 484ms/step - loss: 0.0864 - acc: 0.9714 - val_loss: 0.4955 - val_acc: 0.8655\n",
      "Epoch 27/50\n",
      "50/50 [==============================] - 24s 484ms/step - loss: 0.0817 - acc: 0.9717 - val_loss: 0.5176 - val_acc: 0.8684\n",
      "Epoch 28/50\n",
      "50/50 [==============================] - 24s 484ms/step - loss: 0.0636 - acc: 0.9795 - val_loss: 0.5856 - val_acc: 0.8526\n",
      "Epoch 29/50\n",
      "50/50 [==============================] - 24s 484ms/step - loss: 0.0610 - acc: 0.9793 - val_loss: 0.5568 - val_acc: 0.8298\n",
      "Epoch 30/50\n",
      "50/50 [==============================] - 24s 483ms/step - loss: 0.0488 - acc: 0.9847 - val_loss: 0.6518 - val_acc: 0.8398\n",
      "Epoch 31/50\n",
      "50/50 [==============================] - 25s 496ms/step - loss: 0.0570 - acc: 0.9822 - val_loss: 0.6299 - val_acc: 0.8641\n",
      "Epoch 32/50\n",
      "50/50 [==============================] - 24s 488ms/step - loss: 0.0620 - acc: 0.9851 - val_loss: 0.6272 - val_acc: 0.8698\n",
      "Epoch 33/50\n",
      "50/50 [==============================] - 24s 482ms/step - loss: 0.0620 - acc: 0.9825 - val_loss: 0.5493 - val_acc: 0.8598\n",
      "Epoch 34/50\n",
      "50/50 [==============================] - 24s 479ms/step - loss: 0.0451 - acc: 0.9892 - val_loss: 0.6546 - val_acc: 0.8655\n",
      "Epoch 35/50\n",
      "50/50 [==============================] - 25s 496ms/step - loss: 0.0301 - acc: 0.9932 - val_loss: 0.7051 - val_acc: 0.8784\n",
      "Epoch 36/50\n",
      "50/50 [==============================] - 24s 485ms/step - loss: 0.0558 - acc: 0.9841 - val_loss: 0.6441 - val_acc: 0.8498\n",
      "Epoch 37/50\n",
      "50/50 [==============================] - 24s 475ms/step - loss: 0.0390 - acc: 0.9887 - val_loss: 0.8563 - val_acc: 0.8712\n",
      "Epoch 38/50\n",
      "50/50 [==============================] - 24s 475ms/step - loss: 0.0361 - acc: 0.9895 - val_loss: 0.7085 - val_acc: 0.8383\n",
      "Epoch 39/50\n",
      "50/50 [==============================] - 24s 478ms/step - loss: 0.0369 - acc: 0.9893 - val_loss: 0.7128 - val_acc: 0.8598\n",
      "Epoch 40/50\n",
      "50/50 [==============================] - 24s 475ms/step - loss: 0.0312 - acc: 0.9917 - val_loss: 0.6899 - val_acc: 0.8412\n",
      "Epoch 41/50\n",
      "50/50 [==============================] - 24s 476ms/step - loss: 0.0245 - acc: 0.9941 - val_loss: 0.8091 - val_acc: 0.8698\n",
      "Epoch 42/50\n",
      "50/50 [==============================] - 24s 475ms/step - loss: 0.0324 - acc: 0.9919 - val_loss: 0.7716 - val_acc: 0.8541\n",
      "Epoch 43/50\n",
      "50/50 [==============================] - 24s 481ms/step - loss: 0.0207 - acc: 0.9944 - val_loss: 0.6828 - val_acc: 0.8097\n",
      "Epoch 44/50\n",
      "50/50 [==============================] - 24s 484ms/step - loss: 0.0235 - acc: 0.9933 - val_loss: 0.8956 - val_acc: 0.8040\n",
      "Epoch 45/50\n",
      "50/50 [==============================] - 24s 484ms/step - loss: 0.0214 - acc: 0.9932 - val_loss: 0.8308 - val_acc: 0.8526\n",
      "Epoch 46/50\n",
      "50/50 [==============================] - 24s 480ms/step - loss: 0.0254 - acc: 0.9941 - val_loss: 0.8577 - val_acc: 0.8441\n",
      "Epoch 47/50\n",
      "50/50 [==============================] - 24s 479ms/step - loss: 0.0271 - acc: 0.9913 - val_loss: 0.8141 - val_acc: 0.8541\n",
      "Epoch 48/50\n",
      "50/50 [==============================] - 24s 482ms/step - loss: 0.0075 - acc: 0.9983 - val_loss: 0.8937 - val_acc: 0.8512\n",
      "Epoch 49/50\n",
      "50/50 [==============================] - 24s 479ms/step - loss: 0.0193 - acc: 0.9940 - val_loss: 0.8852 - val_acc: 0.8512\n",
      "Epoch 50/50\n",
      "50/50 [==============================] - 24s 478ms/step - loss: 0.0123 - acc: 0.9967 - val_loss: 0.9423 - val_acc: 0.8698\n",
      "25/25 [==============================] - 1s 54ms/step - loss: 1.0069 - acc: 0.8443\n",
      "Accuracy:84.43%\n"
     ]
    }
   ],
   "source": [
    "# 开始训练\n",
    "model1.fit(X_train, y_train, validation_split=0.1,epochs=50,batch_size=128)\n",
    "result1 = model1.evaluate(X_test, y_test)\n",
    "print('Accuracy:{0:.2%}'.format(result1[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基于栈式 LSTM 的序列分类\n",
    "### 在这个模型中，我们将 3 个 LSTM 层叠在一起，使模型能够学习更高层次的时间表示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model11=Sequential()\n",
    "model11.add(Embedding(num_words,embedding_dim,\n",
    "                    weights=[embedding_matrix],\n",
    "                   input_length=max_tokens,\n",
    "                   trainable=False))\n",
    "model11.add(LSTM(32, return_sequences=True))  # 返回维度为 32 的向量序列\n",
    "model11.add(LSTM(32, return_sequences=True))  # 返回维度为 32 的向量序列\n",
    "model11.add(LSTM(32))  # 返回维度为 32 的单个向量\n",
    "model11.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model11.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_17 (Embedding)     (None, 222, 300)          77964900  \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 222, 32)           42624     \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 222, 32)           8320      \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "module_wrapper_45 (ModuleWra (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 78,024,197\n",
      "Trainable params: 59,297\n",
      "Non-trainable params: 77,964,900\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model11.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "50/50 [==============================] - 36s 717ms/step - loss: 0.4911 - acc: 0.7594 - val_loss: 0.4739 - val_acc: 0.7439\n",
      "Epoch 2/10\n",
      "50/50 [==============================] - 35s 697ms/step - loss: 0.4312 - acc: 0.7979 - val_loss: 0.4073 - val_acc: 0.8255\n",
      "Epoch 3/10\n",
      "50/50 [==============================] - 35s 700ms/step - loss: 0.3934 - acc: 0.8241 - val_loss: 0.4139 - val_acc: 0.8283\n",
      "Epoch 4/10\n",
      "50/50 [==============================] - 34s 683ms/step - loss: 0.3666 - acc: 0.8412 - val_loss: 0.3866 - val_acc: 0.8212\n",
      "Epoch 5/10\n",
      "50/50 [==============================] - 34s 686ms/step - loss: 0.3419 - acc: 0.8507 - val_loss: 0.3482 - val_acc: 0.8455\n",
      "Epoch 6/10\n",
      "50/50 [==============================] - 34s 681ms/step - loss: 0.3241 - acc: 0.8585 - val_loss: 0.3861 - val_acc: 0.8469\n",
      "Epoch 7/10\n",
      "50/50 [==============================] - 33s 660ms/step - loss: 0.3057 - acc: 0.8704 - val_loss: 0.3956 - val_acc: 0.8340\n",
      "Epoch 8/10\n",
      "50/50 [==============================] - 34s 689ms/step - loss: 0.2953 - acc: 0.8733 - val_loss: 0.4055 - val_acc: 0.8255\n",
      "Epoch 9/10\n",
      "50/50 [==============================] - 35s 691ms/step - loss: 0.2760 - acc: 0.8823 - val_loss: 0.5751 - val_acc: 0.7597\n",
      "Epoch 10/10\n",
      "50/50 [==============================] - 35s 691ms/step - loss: 0.2689 - acc: 0.8866 - val_loss: 0.3998 - val_acc: 0.8340\n",
      "25/25 [==============================] - 3s 101ms/step - loss: 0.3401 - acc: 0.8610\n",
      "Accuracy:86.10%\n"
     ]
    }
   ],
   "source": [
    "# 开始训练\n",
    "model11.fit(X_train, y_train, validation_split=0.1,epochs=10,batch_size=128)\n",
    "result11 = model11.evaluate(X_test, y_test)\n",
    "print('Accuracy:{0:.2%}'.format(result11[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#多层感知机训练\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Embedding(num_words,embedding_dim,\n",
    "                    weights=[embedding_matrix],\n",
    "                   input_length=max_tokens, \n",
    "                   trainable=False))\n",
    "model2.add(Dense(64, input_dim=20, activation='relu'))\n",
    "model2.add(Dropout(0.5))\n",
    "model2.add(Dense(64, activation='relu'))\n",
    "model2.add(Dropout(0.5))\n",
    "model2.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model2.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['acc'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_19 (Embedding)     (None, 222, 300)          77964900  \n",
      "_________________________________________________________________\n",
      "module_wrapper_50 (ModuleWra (None, 222, 64)           19264     \n",
      "_________________________________________________________________\n",
      "module_wrapper_51 (ModuleWra (None, 222, 64)           0         \n",
      "_________________________________________________________________\n",
      "module_wrapper_52 (ModuleWra (None, 222, 64)           4160      \n",
      "_________________________________________________________________\n",
      "module_wrapper_53 (ModuleWra (None, 222, 64)           0         \n",
      "_________________________________________________________________\n",
      "module_wrapper_54 (ModuleWra (None, 222, 1)            65        \n",
      "=================================================================\n",
      "Total params: 77,988,389\n",
      "Trainable params: 23,489\n",
      "Non-trainable params: 77,964,900\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "50/50 [==============================] - 6s 107ms/step - loss: 0.6434 - acc: 0.6677 - val_loss: 0.5917 - val_acc: 0.7210\n",
      "Epoch 2/50\n",
      "50/50 [==============================] - 5s 103ms/step - loss: 0.6285 - acc: 0.6824 - val_loss: 0.6276 - val_acc: 0.7188\n",
      "Epoch 3/50\n",
      "50/50 [==============================] - 5s 103ms/step - loss: 0.6249 - acc: 0.6839 - val_loss: 0.5888 - val_acc: 0.7214\n",
      "Epoch 4/50\n",
      "50/50 [==============================] - 5s 104ms/step - loss: 0.6226 - acc: 0.6852 - val_loss: 0.6142 - val_acc: 0.7191\n",
      "Epoch 5/50\n",
      "50/50 [==============================] - 5s 104ms/step - loss: 0.6205 - acc: 0.6859 - val_loss: 0.5853 - val_acc: 0.7203\n",
      "Epoch 6/50\n",
      "50/50 [==============================] - 5s 104ms/step - loss: 0.6214 - acc: 0.6860 - val_loss: 0.5981 - val_acc: 0.7219\n",
      "Epoch 7/50\n",
      "50/50 [==============================] - 5s 104ms/step - loss: 0.6189 - acc: 0.6868 - val_loss: 0.6063 - val_acc: 0.7219\n",
      "Epoch 8/50\n",
      "50/50 [==============================] - 5s 104ms/step - loss: 0.6188 - acc: 0.6869 - val_loss: 0.6056 - val_acc: 0.7198\n",
      "Epoch 9/50\n",
      "50/50 [==============================] - 5s 104ms/step - loss: 0.6186 - acc: 0.6871 - val_loss: 0.5885 - val_acc: 0.7220\n",
      "Epoch 10/50\n",
      "50/50 [==============================] - 5s 105ms/step - loss: 0.6189 - acc: 0.6872 - val_loss: 0.6000 - val_acc: 0.7219\n",
      "Epoch 11/50\n",
      "50/50 [==============================] - 5s 104ms/step - loss: 0.6182 - acc: 0.6874 - val_loss: 0.6058 - val_acc: 0.7208\n",
      "Epoch 12/50\n",
      "50/50 [==============================] - 5s 104ms/step - loss: 0.6173 - acc: 0.6876 - val_loss: 0.5983 - val_acc: 0.7214\n",
      "Epoch 13/50\n",
      "50/50 [==============================] - 5s 103ms/step - loss: 0.6171 - acc: 0.6878 - val_loss: 0.5831 - val_acc: 0.7222\n",
      "Epoch 14/50\n",
      "50/50 [==============================] - 5s 103ms/step - loss: 0.6177 - acc: 0.6878 - val_loss: 0.5912 - val_acc: 0.7220\n",
      "Epoch 15/50\n",
      "50/50 [==============================] - 5s 104ms/step - loss: 0.6172 - acc: 0.6879 - val_loss: 0.5842 - val_acc: 0.7223\n",
      "Epoch 16/50\n",
      "50/50 [==============================] - 5s 104ms/step - loss: 0.6173 - acc: 0.6879 - val_loss: 0.5877 - val_acc: 0.7220\n",
      "Epoch 17/50\n",
      "50/50 [==============================] - 5s 104ms/step - loss: 0.6163 - acc: 0.6882 - val_loss: 0.5941 - val_acc: 0.7210\n",
      "Epoch 18/50\n",
      "50/50 [==============================] - 5s 103ms/step - loss: 0.6166 - acc: 0.6880 - val_loss: 0.5904 - val_acc: 0.7210\n",
      "Epoch 19/50\n",
      "50/50 [==============================] - 5s 104ms/step - loss: 0.6160 - acc: 0.6880 - val_loss: 0.5894 - val_acc: 0.7222\n",
      "Epoch 20/50\n",
      "50/50 [==============================] - 5s 103ms/step - loss: 0.6158 - acc: 0.6880 - val_loss: 0.5863 - val_acc: 0.7223\n",
      "Epoch 21/50\n",
      "50/50 [==============================] - 5s 103ms/step - loss: 0.6154 - acc: 0.6883 - val_loss: 0.5877 - val_acc: 0.7222\n",
      "Epoch 22/50\n",
      "50/50 [==============================] - 5s 104ms/step - loss: 0.6155 - acc: 0.6882 - val_loss: 0.5881 - val_acc: 0.7220\n",
      "Epoch 23/50\n",
      "50/50 [==============================] - 5s 104ms/step - loss: 0.6152 - acc: 0.6882 - val_loss: 0.5878 - val_acc: 0.7223\n",
      "Epoch 24/50\n",
      "50/50 [==============================] - 5s 103ms/step - loss: 0.6152 - acc: 0.6883 - val_loss: 0.5863 - val_acc: 0.7221\n",
      "Epoch 25/50\n",
      "50/50 [==============================] - 5s 103ms/step - loss: 0.6151 - acc: 0.6884 - val_loss: 0.5888 - val_acc: 0.7221\n",
      "Epoch 26/50\n",
      "50/50 [==============================] - 5s 104ms/step - loss: 0.6150 - acc: 0.6885 - val_loss: 0.5880 - val_acc: 0.7221\n",
      "Epoch 27/50\n",
      "50/50 [==============================] - 5s 104ms/step - loss: 0.6149 - acc: 0.6884 - val_loss: 0.5862 - val_acc: 0.7222\n",
      "Epoch 28/50\n",
      "50/50 [==============================] - 5s 104ms/step - loss: 0.6149 - acc: 0.6883 - val_loss: 0.5880 - val_acc: 0.7223\n",
      "Epoch 29/50\n",
      "50/50 [==============================] - 5s 103ms/step - loss: 0.6149 - acc: 0.6883 - val_loss: 0.5890 - val_acc: 0.7223\n",
      "Epoch 30/50\n",
      "50/50 [==============================] - 5s 103ms/step - loss: 0.6148 - acc: 0.6885 - val_loss: 0.5871 - val_acc: 0.7222\n",
      "Epoch 31/50\n",
      "50/50 [==============================] - 5s 103ms/step - loss: 0.6147 - acc: 0.6884 - val_loss: 0.5871 - val_acc: 0.7224\n",
      "Epoch 32/50\n",
      "50/50 [==============================] - 5s 103ms/step - loss: 0.6147 - acc: 0.6885 - val_loss: 0.5874 - val_acc: 0.7224\n",
      "Epoch 33/50\n",
      "50/50 [==============================] - 5s 104ms/step - loss: 0.6145 - acc: 0.6887 - val_loss: 0.5855 - val_acc: 0.7223\n",
      "Epoch 34/50\n",
      "50/50 [==============================] - 5s 103ms/step - loss: 0.6146 - acc: 0.6887 - val_loss: 0.5873 - val_acc: 0.7224\n",
      "Epoch 35/50\n",
      "50/50 [==============================] - 5s 104ms/step - loss: 0.6145 - acc: 0.6885 - val_loss: 0.5857 - val_acc: 0.7222\n",
      "Epoch 36/50\n",
      "50/50 [==============================] - 5s 103ms/step - loss: 0.6146 - acc: 0.6885 - val_loss: 0.5870 - val_acc: 0.7223\n",
      "Epoch 37/50\n",
      "50/50 [==============================] - 5s 104ms/step - loss: 0.6144 - acc: 0.6886 - val_loss: 0.5847 - val_acc: 0.7225\n",
      "Epoch 38/50\n",
      "50/50 [==============================] - 5s 104ms/step - loss: 0.6146 - acc: 0.6885 - val_loss: 0.5861 - val_acc: 0.7226\n",
      "Epoch 39/50\n",
      "50/50 [==============================] - 5s 103ms/step - loss: 0.6144 - acc: 0.6886 - val_loss: 0.5856 - val_acc: 0.7225\n",
      "Epoch 40/50\n",
      "50/50 [==============================] - 5s 104ms/step - loss: 0.6145 - acc: 0.6885 - val_loss: 0.5861 - val_acc: 0.7225\n",
      "Epoch 41/50\n",
      "50/50 [==============================] - 5s 104ms/step - loss: 0.6145 - acc: 0.6885 - val_loss: 0.5862 - val_acc: 0.7225\n",
      "Epoch 42/50\n",
      "50/50 [==============================] - 5s 103ms/step - loss: 0.6143 - acc: 0.6886 - val_loss: 0.5872 - val_acc: 0.7222\n",
      "Epoch 43/50\n",
      "50/50 [==============================] - 5s 103ms/step - loss: 0.6143 - acc: 0.6886 - val_loss: 0.5898 - val_acc: 0.7220\n",
      "Epoch 44/50\n",
      "50/50 [==============================] - 5s 104ms/step - loss: 0.6143 - acc: 0.6888 - val_loss: 0.5888 - val_acc: 0.7221\n",
      "Epoch 45/50\n",
      "50/50 [==============================] - 5s 104ms/step - loss: 0.6141 - acc: 0.6888 - val_loss: 0.5864 - val_acc: 0.7220\n",
      "Epoch 46/50\n",
      "50/50 [==============================] - 5s 103ms/step - loss: 0.6141 - acc: 0.6888 - val_loss: 0.5855 - val_acc: 0.7225\n",
      "Epoch 47/50\n",
      "50/50 [==============================] - 5s 103ms/step - loss: 0.6141 - acc: 0.6887 - val_loss: 0.5854 - val_acc: 0.7227\n",
      "Epoch 48/50\n",
      "50/50 [==============================] - 5s 104ms/step - loss: 0.6141 - acc: 0.6885 - val_loss: 0.5867 - val_acc: 0.7225\n",
      "Epoch 49/50\n",
      "50/50 [==============================] - 5s 103ms/step - loss: 0.6140 - acc: 0.6887 - val_loss: 0.5867 - val_acc: 0.7223\n",
      "Epoch 50/50\n",
      "50/50 [==============================] - 5s 104ms/step - loss: 0.6139 - acc: 0.6888 - val_loss: 0.5878 - val_acc: 0.7223\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.6202 - acc: 0.6775\n",
      "Accuracy:67.75%\n"
     ]
    }
   ],
   "source": [
    "model2.fit(X_train, y_train, validation_split=0.1,epochs=50,batch_size=128)\n",
    "result2 = model2.evaluate(X_test, y_test)\n",
    "print('Accuracy:{0:.2%}'.format(result2[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.recurrent import SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用simple-rnn对样本进行分类训练\n",
    "model3 = Sequential()\n",
    "# 模型第一层为embedding\n",
    "model3.add(Embedding(num_words,embedding_dim,\n",
    "                    weights=[embedding_matrix],\n",
    "                   input_length=max_tokens,\n",
    "                   trainable=False))\n",
    "model3.add(SimpleRNN(units=32))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# 最后是一个全链接层，用sigmoid激活函数输出结果。\n",
    "model3.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model3.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_13 (Embedding)     (None, 222, 300)          77964900  \n",
      "_________________________________________________________________\n",
      "module_wrapper_39 (ModuleWra (None, 32)                10656     \n",
      "_________________________________________________________________\n",
      "module_wrapper_40 (ModuleWra (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 77,975,589\n",
      "Trainable params: 10,689\n",
      "Non-trainable params: 77,964,900\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "50/50 [==============================] - 5s 65ms/step - loss: 0.6130 - acc: 0.6653 - val_loss: 0.5524 - val_acc: 0.7110\n",
      "Epoch 2/50\n",
      "50/50 [==============================] - 3s 62ms/step - loss: 0.5404 - acc: 0.7311 - val_loss: 0.5159 - val_acc: 0.7368\n",
      "Epoch 3/50\n",
      "50/50 [==============================] - 3s 63ms/step - loss: 0.4831 - acc: 0.7699 - val_loss: 0.4704 - val_acc: 0.7854\n",
      "Epoch 4/50\n",
      "50/50 [==============================] - 3s 62ms/step - loss: 0.4529 - acc: 0.7885 - val_loss: 0.4506 - val_acc: 0.8026\n",
      "Epoch 5/50\n",
      "50/50 [==============================] - 3s 62ms/step - loss: 0.4398 - acc: 0.8017 - val_loss: 0.4677 - val_acc: 0.7825\n",
      "Epoch 6/50\n",
      "50/50 [==============================] - 3s 62ms/step - loss: 0.4444 - acc: 0.8014 - val_loss: 0.4538 - val_acc: 0.8026\n",
      "Epoch 7/50\n",
      "50/50 [==============================] - 3s 62ms/step - loss: 0.4090 - acc: 0.8198 - val_loss: 0.4895 - val_acc: 0.7697\n",
      "Epoch 8/50\n",
      "50/50 [==============================] - 3s 62ms/step - loss: 0.4002 - acc: 0.8224 - val_loss: 0.4920 - val_acc: 0.7711\n",
      "Epoch 9/50\n",
      "50/50 [==============================] - 3s 62ms/step - loss: 0.3919 - acc: 0.8270 - val_loss: 0.5247 - val_acc: 0.7840\n",
      "Epoch 10/50\n",
      "50/50 [==============================] - 3s 62ms/step - loss: 0.3842 - acc: 0.8316 - val_loss: 0.5235 - val_acc: 0.7783\n",
      "Epoch 11/50\n",
      "50/50 [==============================] - 3s 63ms/step - loss: 0.3826 - acc: 0.8372 - val_loss: 0.5427 - val_acc: 0.7339\n",
      "Epoch 12/50\n",
      "50/50 [==============================] - 3s 63ms/step - loss: 0.3713 - acc: 0.8386 - val_loss: 0.4867 - val_acc: 0.7954\n",
      "Epoch 13/50\n",
      "50/50 [==============================] - 3s 62ms/step - loss: 0.3658 - acc: 0.8405 - val_loss: 0.4522 - val_acc: 0.8011\n",
      "Epoch 14/50\n",
      "50/50 [==============================] - 3s 62ms/step - loss: 0.3452 - acc: 0.8529 - val_loss: 0.4452 - val_acc: 0.8054\n",
      "Epoch 15/50\n",
      "50/50 [==============================] - 3s 62ms/step - loss: 0.3485 - acc: 0.8475 - val_loss: 0.4522 - val_acc: 0.7897\n",
      "Epoch 16/50\n",
      "50/50 [==============================] - 3s 62ms/step - loss: 0.3404 - acc: 0.8558 - val_loss: 0.4380 - val_acc: 0.8083\n",
      "Epoch 17/50\n",
      "50/50 [==============================] - 3s 62ms/step - loss: 0.3318 - acc: 0.8623 - val_loss: 0.4713 - val_acc: 0.7940\n",
      "Epoch 18/50\n",
      "50/50 [==============================] - 3s 63ms/step - loss: 0.3463 - acc: 0.8504 - val_loss: 0.4714 - val_acc: 0.7840\n",
      "Epoch 19/50\n",
      "50/50 [==============================] - 3s 63ms/step - loss: 0.3067 - acc: 0.8763 - val_loss: 0.4766 - val_acc: 0.7969\n",
      "Epoch 20/50\n",
      "50/50 [==============================] - 3s 62ms/step - loss: 0.3329 - acc: 0.8585 - val_loss: 0.4450 - val_acc: 0.8155\n",
      "Epoch 21/50\n",
      "50/50 [==============================] - 3s 63ms/step - loss: 0.3044 - acc: 0.8763 - val_loss: 0.5084 - val_acc: 0.7797\n",
      "Epoch 22/50\n",
      "50/50 [==============================] - 3s 62ms/step - loss: 0.3067 - acc: 0.8709 - val_loss: 0.4651 - val_acc: 0.7897\n",
      "Epoch 23/50\n",
      "50/50 [==============================] - 3s 62ms/step - loss: 0.2970 - acc: 0.8803 - val_loss: 0.5576 - val_acc: 0.7196\n",
      "Epoch 24/50\n",
      "50/50 [==============================] - 3s 62ms/step - loss: 0.3023 - acc: 0.8795 - val_loss: 0.4690 - val_acc: 0.8097\n",
      "Epoch 25/50\n",
      "50/50 [==============================] - 3s 63ms/step - loss: 0.2994 - acc: 0.8777 - val_loss: 0.5668 - val_acc: 0.7339\n",
      "Epoch 26/50\n",
      "50/50 [==============================] - 3s 62ms/step - loss: 0.2536 - acc: 0.9025 - val_loss: 0.6417 - val_acc: 0.7639\n",
      "Epoch 27/50\n",
      "50/50 [==============================] - 3s 62ms/step - loss: 0.2726 - acc: 0.8903 - val_loss: 0.4888 - val_acc: 0.8069\n",
      "Epoch 28/50\n",
      "50/50 [==============================] - 3s 63ms/step - loss: 0.2812 - acc: 0.8882 - val_loss: 0.5951 - val_acc: 0.7496\n",
      "Epoch 29/50\n",
      "50/50 [==============================] - 3s 62ms/step - loss: 0.2518 - acc: 0.9044 - val_loss: 0.6432 - val_acc: 0.7768\n",
      "Epoch 30/50\n",
      "50/50 [==============================] - 3s 63ms/step - loss: 0.2721 - acc: 0.8952 - val_loss: 0.5191 - val_acc: 0.7797\n",
      "Epoch 31/50\n",
      "50/50 [==============================] - 3s 63ms/step - loss: 0.2575 - acc: 0.9036 - val_loss: 0.5184 - val_acc: 0.7926\n",
      "Epoch 32/50\n",
      "50/50 [==============================] - 3s 62ms/step - loss: 0.2434 - acc: 0.9065 - val_loss: 0.5384 - val_acc: 0.7969\n",
      "Epoch 33/50\n",
      "50/50 [==============================] - 3s 62ms/step - loss: 0.2445 - acc: 0.9055 - val_loss: 0.5169 - val_acc: 0.7926\n",
      "Epoch 34/50\n",
      "50/50 [==============================] - 3s 63ms/step - loss: 0.2397 - acc: 0.9078 - val_loss: 0.5631 - val_acc: 0.7568\n",
      "Epoch 35/50\n",
      "50/50 [==============================] - 3s 62ms/step - loss: 0.2190 - acc: 0.9186 - val_loss: 0.5369 - val_acc: 0.7969\n",
      "Epoch 36/50\n",
      "50/50 [==============================] - 3s 63ms/step - loss: 0.2533 - acc: 0.9044 - val_loss: 0.5367 - val_acc: 0.7997\n",
      "Epoch 37/50\n",
      "50/50 [==============================] - 3s 63ms/step - loss: 0.2078 - acc: 0.9280 - val_loss: 0.5567 - val_acc: 0.7926\n",
      "Epoch 38/50\n",
      "50/50 [==============================] - 3s 62ms/step - loss: 0.2062 - acc: 0.9235 - val_loss: 0.5619 - val_acc: 0.7954\n",
      "Epoch 39/50\n",
      "50/50 [==============================] - 3s 62ms/step - loss: 0.1976 - acc: 0.9269 - val_loss: 0.6452 - val_acc: 0.7883\n",
      "Epoch 40/50\n",
      "50/50 [==============================] - 3s 62ms/step - loss: 0.2285 - acc: 0.9122 - val_loss: 0.5581 - val_acc: 0.7597\n",
      "Epoch 41/50\n",
      "50/50 [==============================] - 3s 63ms/step - loss: 0.1765 - acc: 0.9374 - val_loss: 0.6022 - val_acc: 0.7868\n",
      "Epoch 42/50\n",
      "50/50 [==============================] - 3s 63ms/step - loss: 0.2005 - acc: 0.9281 - val_loss: 0.6932 - val_acc: 0.7768\n",
      "Epoch 43/50\n",
      "50/50 [==============================] - 3s 62ms/step - loss: 0.1813 - acc: 0.9396 - val_loss: 0.6462 - val_acc: 0.7711\n",
      "Epoch 44/50\n",
      "50/50 [==============================] - 3s 63ms/step - loss: 0.1671 - acc: 0.9434 - val_loss: 0.6230 - val_acc: 0.7454\n",
      "Epoch 45/50\n",
      "50/50 [==============================] - 3s 63ms/step - loss: 0.1761 - acc: 0.9402 - val_loss: 0.6424 - val_acc: 0.7682\n",
      "Epoch 46/50\n",
      "50/50 [==============================] - 3s 62ms/step - loss: 0.1687 - acc: 0.9426 - val_loss: 0.6530 - val_acc: 0.7783\n",
      "Epoch 47/50\n",
      "50/50 [==============================] - 3s 62ms/step - loss: 0.1972 - acc: 0.9321 - val_loss: 0.6867 - val_acc: 0.7639\n",
      "Epoch 48/50\n",
      "50/50 [==============================] - 3s 63ms/step - loss: 0.1413 - acc: 0.9558 - val_loss: 0.6722 - val_acc: 0.7725\n",
      "Epoch 49/50\n",
      "50/50 [==============================] - 3s 63ms/step - loss: 0.1535 - acc: 0.9528 - val_loss: 0.8359 - val_acc: 0.7425\n",
      "Epoch 50/50\n",
      "50/50 [==============================] - 3s 63ms/step - loss: 0.1985 - acc: 0.9308 - val_loss: 0.6764 - val_acc: 0.7797\n",
      "25/25 [==============================] - 0s 17ms/step - loss: 0.6868 - acc: 0.7606\n",
      "Accuracy:76.06%\n"
     ]
    }
   ],
   "source": [
    "model3.fit(X_train, y_train, validation_split=0.1,epochs=50,batch_size=128)\n",
    "result3 = model3.evaluate(X_test, y_test)\n",
    "print('Accuracy:{0:.2%}'.format(result3[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
